{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Next Ionogram Frame  (2).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Artikasaridewi/data/blob/main/Next_Ionogram_Frame_(2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project Description: \n",
        "\n",
        "Oleh: Varuliantor Dear, Annis Siradj M, Artika, dan Yana Robiana\n",
        "\n",
        "Next Ionogram Plot Prediction versi 2 GRAY IMAGE\n",
        "\n",
        "Sumber: https://keras.io/examples/vision/conv_lstm/#dataset-construction  dengan sedikit modifikasi seperti import model dan lain lain\n",
        "\n",
        "Input: Dataset berupa frame ionogram per 15 menit dallam satu hari selama satu bulan\n",
        "\n",
        "Output: Next Frame Ionogram\n",
        "\n",
        "Target tahun 2022"
      ],
      "metadata": {
        "id": "vS5T-Nov5C3r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IMPORT DATA FROM DRIVE"
      ],
      "metadata": {
        "id": "0XNQXDQfsT2g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from matplotlib import pyplot as plt\n",
        "import cv2\n",
        "import random\n"
      ],
      "metadata": {
        "id": "yAGOG-b_5K6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IMPORT DATA FROM GITHUB"
      ],
      "metadata": {
        "id": "9QBde5EkXxcc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/varuliant/Ionogram_KOE.git  "
      ],
      "metadata": {
        "id": "xTbxFP_cXzUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "C4J4NCi9-8I5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os"
      ],
      "metadata": {
        "id": "QvCcHA2U_hOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --no-check-certificate \\\n",
        "    \"https://github.com/varuliant/Ionogram_KOE.git\" \\\n",
        "    -O \"/tmp/cats-and-dogs.zip\"\n",
        "\n",
        "\n",
        "zip_ref = zipfile.ZipFile('/tmp/cats-and-dogs.zip', 'r') #Opens the zip file in read mode\n",
        "zip_ref.extractall('/tmp') #Extracts the files into the /tmp folder\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "id": "mxRf5zjI_iSt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test data apakah tetrsimpan\n",
        "import os\n",
        "from google.colab import drive\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Path Data\n",
        "path = \"/content/Ionogram_KOE/Clean/220401KO/\"   # Gunakan tanda quote 2 \n",
        "\n",
        "#print(os.listdir(path))\n",
        "image = cv2.imread(path + os.listdir(path)[0])\n",
        "type(image)\n",
        "#print(os.listdir(path)[0])\n",
        "#print(image)\n",
        "plt.imshow(image)"
      ],
      "metadata": {
        "id": "IqlwkFzINg3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(image.shape)"
      ],
      "metadata": {
        "id": "lcO2FXMhSRmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GENERATE Dataset dari Folder dan Sub Folder sendiri\n",
        "Terdiri dari:\n",
        "1. Fungsi Membaca Image atau frame pada Sub folder dan mengubahnya kedalam array 4D\n",
        "2. Iterasi per sub folder dari main folder sehingga menjadi array 5 D\n",
        "yang dikelompokkan kedalam sub folder \n",
        "\n",
        "Syarat: Jumlah frame dalam sub folder harus sama. Contoh Ionogram 1 hari 96 image\n",
        "\n",
        "Opsi untuk performa:\n",
        "1. Croping wilayah jejak ionogram tanpa axis secara konsisten\n",
        "2. Grayscale image\n",
        "3. Clean ionogram traces"
      ],
      "metadata": {
        "id": "tHazALVSr1fK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from os import listdir\n",
        "from os.path import isdir\n",
        "import cv2\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "VO9_SvuA0gpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MENYIAPKAN DATA SET DARI FOLDER PER HARI (FINAL)"
      ],
      "metadata": {
        "id": "PZakP43QyAZB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fungsi ini dipilih karena ada fungsi sort nama file agar berurutan\n",
        "def ambilfile2(sub_dir):\n",
        "  y=70\n",
        "  x=110\n",
        "  h=380\n",
        "  w=400\n",
        "  width = 80\n",
        "  height = 80\n",
        "  images_sub = np.empty(shape=(0, height, width,1), dtype='uint8') \n",
        "  #images_sub=[]\n",
        "  names = os.listdir(sub_dir)\n",
        "  names.sort()\n",
        "  for filename in names:\n",
        "    image=cv2.imread(sub_dir + '/' + filename)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) # image langsung sebagai gray\n",
        "    #image = cv2.bitwise_not(image)  # invert the B n W to be Black Background\n",
        "    # bisa juga gunakan hanya merah saja agar jejak ionogram jelas # split the image into its three channels\n",
        "    # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    # (R,G,B) = cv2.split(image)\n",
        "    #image = R\n",
        "    # UNTUK GRAY SCALE background Black ###################\n",
        "    #se = cv2.getStructuringElement(cv2.MORPH_RECT , (8,8))\n",
        "    #bg = cv2.morphologyEx(image, cv2.MORPH_DILATE, se)\n",
        "    #image = cv2.divide(image, bg, scale=255)\n",
        "    #image = cv2.threshold(image, 127, 255, cv2.THRESH_OTSU )[1] \n",
        "    #(thresh, blackAndWhiteImage) = cv2.threshold(grayImage, 127, 255, cv2.THRESH_BINARY)\n",
        "    #######################################\n",
        "    image = image[y:y+h, x:x+w] # Trik untuk croping wilayah ionogram tertentu\n",
        "    image = cv2.resize(image, (height, width))\n",
        "    image = image[..., np.newaxis]#tambahkan extra array agar sesuai dengan feeder model\n",
        "    #print (image.shape)\n",
        "    images_sub = np.append(images_sub, [image/255], axis=0) # ini dat per sub folder\n",
        "  return images_sub \n",
        "\n",
        "#Untuk test panggil\n",
        "#subdirr=\"/content/Ionogram_KOE/Clean/220401KO/\"\n",
        "#dataset_frame=ambilfile(subdirr)\n",
        "#print (dataset_frame.shape)"
      ],
      "metadata": {
        "id": "FQWmclNSwsH1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from os import listdir\n",
        "from os.path import isdir\n",
        "import cv2\n",
        "import numpy as np\n",
        "# LOPPING UTAMA UNTUK PREPARING DATASET DARI FOLDER UTAMA\n",
        "#main_dir=\"/content/Ionogram_KOE/Original/\"\n",
        "main_dir=\"/content/Ionogram_KOE/Clean/\"\n",
        "\n",
        "\n",
        "dataframe = [] # empty list of dataframe\n",
        "for subdir in listdir(main_dir):\n",
        "  #print(subdir)\n",
        "  path_sub_dir=main_dir + subdir\n",
        "  #clling sub dir\n",
        "  subfolder_frame=ambilfile2(path_sub_dir)\n",
        "  \n",
        "  if not isdir(main_dir):\n",
        "    print(path_sub_dir)\n",
        "    continue\n",
        "  # subfolder_frame = subfolder_frame[np.newaxis, ...] # buat menjadi array dimensi baru\n",
        "  # Tumpuk menjadi array dimensi baru dari subfolder_frame\n",
        "  # dataset_frame = np.append(dataset_frame, subfolder_frame, axis=0) # ini harus di deklarasikan dulu shape awalnya dan harus sama ukurannya\n",
        "  list_sub = subfolder_frame.tolist() # ubah kedalam list dari array nupy\n",
        "  dataframe.append(list_sub)\n",
        "dataset_frame = np.asarray(dataframe)\n",
        "  \n",
        "# OK Syarat Data frame per subfolder harus memiliki jumlah yang sama agar tidak eror  "
      ],
      "metadata": {
        "id": "ao8g4nX514lo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(listdir(main_dir))"
      ],
      "metadata": {
        "id": "d6kd4BQAiifz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Periksa susunan dimensi array dataset per frame\n",
        "print (subfolder_frame.shape)\n",
        "print (len(list_sub))\n",
        "print (len(dataframe))\n",
        "print (dataset_frame.shape)"
      ],
      "metadata": {
        "id": "PAFzu3ws31NT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DEFINISI ULANG JUMLAH DATFRAME SHAPE AGAR TIDAK CRASH AKIBAT OVERLOAD RAM"
      ],
      "metadata": {
        "id": "Sl08WFJd6IhX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definisi ulang dataset shape dari 96 image menajdi 24 image\n",
        "dataset_frame=dataset_frame[:, 24:48, :, :]\n",
        "print(dataset_frame.shape)"
      ],
      "metadata": {
        "id": "OpcNUTH1zDEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MENYUSUN DATA SET UNTUK TRAINING DAN VALIDASI"
      ],
      "metadata": {
        "id": "B9oVIpYSQFi-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into train and validation sets using indexing to optimize memory.\n",
        "indexes = np.arange(dataset_frame.shape[0])\n",
        "np.random.shuffle(indexes)\n",
        "train_index = indexes[: int(0.9 * dataset_frame.shape[0])]\n",
        "val_index = indexes[int(0.9 * dataset_frame.shape[0]) :]\n",
        "train_dataset = dataset_frame[train_index]\n",
        "val_dataset = dataset_frame[val_index]\n",
        "\n",
        "\n",
        "# Normalize the data to the 0-1 range untuk GRAY SCALE\n",
        "#train_dataset = train_dataset / 255\n",
        "#val_dataset = val_dataset / 255\n",
        "\n",
        "# We'll define a helper function to shift the frames, where\n",
        "# `x` is frames 0 to n - 1, and `y` is frames 1 to n.\n",
        "def create_shifted_frames(data):\n",
        "    x = data[:, 0 : data.shape[1] - 1, :, :]\n",
        "    y = data[:, 1 : data.shape[1], :, :]\n",
        "    return x, y\n",
        "\n"
      ],
      "metadata": {
        "id": "-ZLZg18-QPqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset_frame.shape)"
      ],
      "metadata": {
        "id": "jhhi3VcdyIp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the processing function to the datasets.\n",
        "x_train, y_train = create_shifted_frames(train_dataset)\n",
        "x_val, y_val = create_shifted_frames(val_dataset)\n",
        "\n",
        "# Inspect the dataset.\n",
        "print(\"Training Dataset Shapes: \" + str(x_train.shape) + \", \" + str(y_train.shape))\n",
        "print(\"Validation Dataset Shapes: \" + str(x_val.shape) + \", \" + str(y_val.shape))"
      ],
      "metadata": {
        "id": "nHsoBrtGQmU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Memeriksa Dataset Frame yang telah disusun"
      ],
      "metadata": {
        "id": "zI3eTpj_Pndh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "indexes = np.arange(dataset_frame.shape[0])\n",
        "\n",
        "# Construct a figure on which we will visualize the images.\n",
        "fig, axes = plt.subplots(4, 6, figsize=(10, 8))\n",
        "\n",
        "# Plot each of the sequential images for one random data example.\n",
        "data_choice = np.random.choice(range(len(train_dataset)), size=1)[0]\n",
        "for idx, ax in enumerate(axes.flat):\n",
        "    ax.imshow(np.squeeze(train_dataset[data_choice][idx]),cmap='gray') # tambahan cmap-'gray' agar tampil seperti plot yang benar\n",
        "    ax.set_title(f\"Frame {idx + 1}\")\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "# Print information and display the figure.\n",
        "print(f\"Displaying frames for example {data_choice}.\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EF4sIU3YPmrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KONSTRUKSI MODEL "
      ],
      "metadata": {
        "id": "WrF-KlE3SuXm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n"
      ],
      "metadata": {
        "id": "Gx1yc9TlS6JY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Construct the input layer with no definite frame size.\n",
        "inp = layers.Input(shape=(None, *x_train.shape[2:]))\n",
        "\n",
        "# We will construct 3 `ConvLSTM2D` layers with batch normalization,\n",
        "# followed by a `Conv3D` layer for the spatiotemporal outputs.\n",
        "x = layers.ConvLSTM2D(filters=64, kernel_size=(5, 5), padding=\"same\", return_sequences=True, activation=\"relu\",)(inp)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.ConvLSTM2D(    filters=64,    kernel_size=(3, 3),    padding=\"same\",    return_sequences=True, activation=\"relu\",)(x)\n",
        "#x = layers.UpSampling2D((2,2))(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.ConvLSTM2D( filters=64, kernel_size=(1, 1), padding=\"same\", return_sequences=True, activation=\"relu\",)(x)\n",
        "x = layers.Conv3D( filters=1, kernel_size=(3, 3, 3), activation=\"sigmoid\", padding=\"same\")(x)  # original filters adalah1 karena untuk gray sclale. kita ubah 3\n",
        "\n",
        "# Next, we will build the complete model and compile it.\n",
        "model = keras.models.Model(inp, x)\n",
        "model.compile(loss=keras.losses.binary_crossentropy, optimizer=keras.optimizers.Adam(),)"
      ],
      "metadata": {
        "id": "QH58xYA5S0jT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.summary())"
      ],
      "metadata": {
        "id": "_KmhtTB7T2od"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TRAINING MODEL"
      ],
      "metadata": {
        "id": "Fia4VBiaTDuY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define some callbacks to improve training.\n",
        "early_stopping = keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=10)\n",
        "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5)\n",
        "\n",
        "# Define modifiable training hyperparameters.\n",
        "epochs = 1000\n",
        "batch_size = 5\n",
        "\n",
        "# Fit the model to the training data.\n",
        "history=model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_val, y_val), callbacks=[early_stopping, reduce_lr],)"
      ],
      "metadata": {
        "id": "cO3EogOxTFUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simpan hasil model training yang dilakukan\n",
        "# sumber: https://www.tensorflow.org/guide/keras/save_and_serialize\n",
        "# Calling `save('my_model.h5')` creates a h5 file `my_model.h5`.\n",
        "model.save(\"my_h5_model_1gray.h5\")"
      ],
      "metadata": {
        "id": "GV0A812dYPF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Panggil model\n",
        "filepath = 'my_h5_model_1gray.h5'"
      ],
      "metadata": {
        "id": "SgFH_v9bFAIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_dict = history.history \n",
        "print(history_dict.keys())"
      ],
      "metadata": {
        "id": "HZ9CaVqlrNlr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(history.history['loss'])\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('LOSS')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R4KYx9rRrYxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PREDIKSI NEXT FRAME DARI DATASET YANG DIPILIH SECARA ACAK"
      ],
      "metadata": {
        "id": "_yN8oykix9EX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select a random example from the validation dataset.\n",
        "example = val_dataset[np.random.choice(range(len(val_dataset)), size=1)[0]]\n",
        "\n",
        "# Pick the first/last ten frames from the example.\n",
        "frames = example[:12, ...]\n",
        "original_frames = example[12:, ...]\n",
        "\n",
        "# Predict a new set of 12 frames.\n",
        "for _ in range(12):\n",
        "    # Extract the model's prediction and post-process it.\n",
        "    new_prediction = model.predict(np.expand_dims(frames, axis=0))\n",
        "    new_prediction = np.squeeze(new_prediction, axis=0)\n",
        "    predicted_frame = np.expand_dims(new_prediction[-1, ...], axis=0)\n",
        "\n",
        "    # Extend the set of prediction frames.\n",
        "    frames = np.concatenate((frames, predicted_frame), axis=0)\n",
        "\n",
        "# Construct a figure for the original and new frames.\n",
        "fig, axes = plt.subplots(2, 12, figsize=(24, 4))\n",
        "\n",
        "# Plot the original frames.\n",
        "for idx, ax in enumerate(axes[0]):\n",
        "    ax.imshow(np.squeeze(original_frames[idx]),cmap='gray')\n",
        "    ax.set_title(f\"Frame {idx + 13}\")\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "# Plot the new frames.\n",
        "new_frames = frames[12:, ...]\n",
        "for idx, ax in enumerate(axes[1]):\n",
        "    ax.imshow(np.squeeze(new_frames[idx]),cmap='gray')\n",
        "    ax.set_title(f\"Frame {idx + 13}\")\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "# Display the figure.\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kXGbexHOSCLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PERIKSA GAMBAR HASIL PREDIKSI SECARA MANUAL \n",
        "predicted_frame.shape\n",
        "plt.imshow(np.squeeze(predicted_frame),cmap='gray')\n",
        "#plt.show()\n"
      ],
      "metadata": {
        "id": "JdbCDJzKZtCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BUAT VIDEO FRAME BY FRAME "
      ],
      "metadata": {
        "id": "U2n3Mb6svR5K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import imageio\n",
        "from IPython.display import Image, display\n",
        "from ipywidgets import widgets, Layout, HBox"
      ],
      "metadata": {
        "id": "uipIq7PYSkP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select a few random examples from the dataset.\n",
        "examples = val_dataset[np.random.choice(range(len(val_dataset)), size=5)]\n",
        "\n",
        "# Iterate over the examples and predict the frames.\n",
        "predicted_videos = []\n",
        "for example in examples:\n",
        "    # Pick the first/last ten frames from the example.\n",
        "    frames = example[:12, ...]\n",
        "    original_frames = example[12:, ...]\n",
        "    new_predictions = np.zeros(shape=(12, *frames[0].shape))\n",
        "\n",
        "    # Predict a new set of 12 frames.\n",
        "    for i in range(12):\n",
        "        # Extract the model's prediction and post-process it.\n",
        "        frames = example[: 12 + i + 1, ...]\n",
        "        new_prediction = model.predict(np.expand_dims(frames, axis=0))\n",
        "        new_prediction = np.squeeze(new_prediction, axis=0)\n",
        "        predicted_frame = np.expand_dims(new_prediction[-1, ...], axis=0)\n",
        "\n",
        "        # Extend the set of prediction frames.\n",
        "        new_predictions[i] = predicted_frame\n",
        "\n",
        "    # Create and save GIFs for each of the ground truth/prediction images.\n",
        "    for frame_set in [original_frames, new_predictions]:\n",
        "        # Construct a GIF from the selected video frames.\n",
        "        current_frames = np.squeeze(frame_set)\n",
        "        current_frames = current_frames[..., np.newaxis] * np.ones(3)\n",
        "        current_frames = (current_frames * 255).astype(np.uint8)\n",
        "        current_frames = list(current_frames)\n",
        "\n",
        "        # Construct a GIF from the frames.\n",
        "        with io.BytesIO() as gif:\n",
        "            imageio.mimsave(gif, current_frames, \"GIF\", fps=5)\n",
        "            predicted_videos.append(gif.getvalue())\n",
        "\n",
        "# Display the videos.\n",
        "print(\" Truth\\tPrediction\")\n",
        "for i in range(0, len(predicted_videos), 2):\n",
        "    # Construct and display an `HBox` with the ground truth and prediction.\n",
        "    box = HBox(\n",
        "        [\n",
        "            widgets.Image(value=predicted_videos[i]),\n",
        "            widgets.Image(value=predicted_videos[i + 1]),\n",
        "        ]\n",
        "    )\n",
        "    display(box)"
      ],
      "metadata": {
        "id": "BO9O2dlvSYSP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reset Colab untuk pengujian berikutnya "
      ],
      "metadata": {
        "id": "CxS-5-_IDLm3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, signal\n",
        "os.kill(os.getpid(), signal.SIGKILL)"
      ],
      "metadata": {
        "id": "aJWcav38Lv8Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}